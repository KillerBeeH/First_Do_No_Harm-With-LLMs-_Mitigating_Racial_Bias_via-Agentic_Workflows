# First_Do_No_Harm-With-LLMs-_Mitigating_Racial_Bias_via-Agentic_Workflows

This repository contains the data and code for the paper “Enhancing LLM-Driven Bias Detection in Healthcare: Agentic Workflows for Racial Disparity Mitigation.”

## Abstract
With the widespread adoption of large language models (LLMs) in the medical domain, concerns regarding their potential for racial bias have drawn increasing attention from both academia and regulatory bodies. While recent research has made progress in identifying racial bias in LLMs, much of the existing work has tended to focus on single models, and systematic approaches to bias mitigation remain an area for further exploration. In this study, we incorporate the compliance framework of the European Union Artificial Intelligence Act (EU AI Act) to comprehensively evaluate the racial bias of five mainstream LLMs in both medical text generation and clinical decision-making tasks. Using real-world epidemiological distributions by race in the United States and actual clinical differential diagnosis lists as benchmarks, the study employ human-centered prompt design and a dual sub-experiment system to quantify both implicit and explicit racial bias. Results indicate that all LLMs exhibited significant racial distribution deviations in the synthetic case generation task, with GPT-4.1 showing the lowest level of implicit bias. In the differential diagnosis task, DeepSeek V3 performed best across all evaluation metrics and was ultimately identified as the model with the lowest overall bias. Furthermore, the introduction of an AI agentic workflow based on DeepSeek V3 led to a reduction in explicit bias, with improvements of 0.0348, 0.1166, and 0.0949 in mean p-value, median p-value, and mean difference, respectively. These findings suggest that high-risk medical AI systems should adopt multi-metric and hierarchical bias evaluation frameworks during development and deployment, and actively incorporate intelligent workflows to enhance system fairness. Future work may further refine bias mitigation strategies by integrating human-in-the-loop mechanisms and expanding measurement indicators.

## Folder Structure

**1. Baseline Data**

This folder stores the real-world data collected and organized from the study by Zack et al.
Their original GitHub repository can be found here:
 https://github.com/elehman16/gpt4_bias

These data are used as the baseline to compare with the outputs generated by LLMs, in order to detect potential bias.

**2. LLMs Output Data**

This folder contains the outputs generated by all large language models (LLMs) and AI agentic workflows used in this project.
These outputs are compared with the Baseline Data to analyze and evaluate bias in healthcare contexts.

**3. Code of Colab / Exp I and Exp II**

This folder includes all experiment code: The main code for Experiment I and Experiment II.

Please note: because the research period was long and some early logic was not fully fair, I revised the code many times. As a result, the overall structure may look a bit complex. Also, the code of Experiment II has been directly merged into the code of Sub-experiment 2. Some code was run again based on earlier runs, with different objects. Because of this, the outputs may look incomplete. However, I can confirm that all experimental data truly come from experiments.

About comments in the code:

I only added comments the first time a key step appears. For repeated or similar parts, I did not add comments again.


 Important Note:
All secret keys related to external resources (e.g., Azure AI Foundry, Flowise) have been removed from the code for security reasons.

**4. Prompts Template**

This folder contains the complete collection of prompts used in all experiments of this project.

**5. Flowise**

This folder contains two figures related to the Flowise agentic workflow:

(1) A screenshot of the agentic workflow pipeline designed in Flowise.
(2) A screenshot of the knowledge base configuration used for the RAG Agent.
